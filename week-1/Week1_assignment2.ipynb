{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##  Linear Regression \n",
                "\n",
                "The objective of this problem is to implement basic linear regression, using both the closed-form solution and gradient descent.\n",
                "\n",
                "\n",
                "* **Dataset:** $\\mathcal{D}=\\{x^{(i)},y^{(i)}\\}_{i=1}^{n}$ where $x^{(i)} \\in \\mathbb{R}^{d}$ and $y^{(i)} \\in \\mathbb{R}$.\n",
                "* **Weights:** $w \\in \\mathbb{R}^{d}$.\n",
                "* **Hypothesis Function:** $h_{w}(x)=w^{T}x$.\n",
                "\n",
                "**Loss Function ($L2$ Loss):**\n",
                "$$ \\mathcal{L}(w)=\\frac{1}{n}\\sum_{i=1}^{N}(y_{i}-w^{T}x^{(i)})^{2} $$\n",
                "\n",
                "**Closed-form Solution:**\n",
                "$$ w^{*}:=(X^{T}X)^{-1}X^{T}y $$\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Function Descriptions\n",
                "\n",
                "* **`train_test_split()`**: This function generates an 80:20 train:test split given a dataset. **You do not need to edit this function.**\n",
                "\n",
                "* **`w_closed_form()`**: Write the code for the closed-form solution of the linear regression problem in this function. \n",
                "\n",
                "* **`l2_loss()`**: Write the code to calculate $L2$ Loss in this function.\n",
                "\n",
                "* **`l2_loss_derivative()`**: Write the code to compute the gradient of the $L2$-Loss function.\n",
                "\n",
                "* **`train_model()`**: Implement gradient descent \n",
                "  * Note that you will have to optimise the `l2_loss` on the train dataset (i.e., optimise `l2_loss(X_train, Y_train, w)`) and update `w` in the direction of decreasing training loss.\n",
                " "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "np.random.seed(335)\n",
                "torch.manual_seed(335)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_test_split(dataframe):\n",
                "    total_samples = dataframe.shape[0]\n",
                "    train_ratio = .8\n",
                "    random_indices = np.random.permutation(total_samples)\n",
                "    train_set_size = int(train_ratio * total_samples)\n",
                "    train_indices = random_indices[:train_set_size]\n",
                "    test_indices = random_indices[train_set_size:]\n",
                "    return dataframe.iloc[train_indices], dataframe.iloc[test_indices]\n",
                "\n",
                "dataset_path = 'dataset01.csv' \n",
                "try:\n",
                "    data = pd.read_csv(dataset_path, index_col=0)\n",
                "    data_train, data_test = train_test_split(data)\n",
                "\n",
                "    X_train = (data_train.iloc[:,:-1].to_numpy())\n",
                "    Y_train = (data_train.iloc[:,-1].to_numpy())\n",
                "    X_train = torch.from_numpy(X_train)\n",
                "    Y_train = torch.from_numpy(Y_train).unsqueeze(1)\n",
                "\n",
                "    X_test = (data_test.iloc[:,:-1].to_numpy())\n",
                "    Y_test = (data_test.iloc[:,-1].to_numpy())\n",
                "    X_test = torch.from_numpy(X_test)\n",
                "    Y_test = torch.from_numpy(Y_test).unsqueeze(1)\n",
                "\n",
                "    d = X_train.shape[1]\n",
                "    print(f\"Data loaded. Dimensions: {d}\")\n",
                "except FileNotFoundError:\n",
                "    print(\"Error: dataset01.csv not found.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Task 1: Closed Form Solution\n",
                "Implement the closed form solution: $w^{*}:=(X^{T}X)^{-1}X^{T}y$.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def w_closed_form(X, Y):\n",
                "    '''\n",
                "    @params\n",
                "        X : 2D tensor of shape(n,d)\n",
                "        Y : 1D tensor of shape(n,1)\n",
                "    function should calculate w_closed : 1D tensor of shape(d,1)\n",
                "    '''\n",
                "   #------------------TODO-------------------------\n",
                "    \n",
                "    w_closed = None # Implement this\n",
                "    return w_closed\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Task 2 & 3: Loss Function and Derivative\n",
                "Implement the $L2$ loss and its derivative with respect to weights $w$.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def l2_loss(X, Y, w):\n",
                "    '''\n",
                "    @params\n",
                "        X : 2D tensor of size(n,d)\n",
                "        Y : 1D tensor of size(n,1)\n",
                "        w : 1D tensor of size(d,1)\n",
                "    return loss : scalar real value\n",
                "    '''\n",
                "    w = w.double()\n",
                "\n",
                "    # ------------TODO------------------    \n",
                "    loss = 0 \n",
                "    return (loss)\n",
                "\n",
                "def l2_loss_derivative(X, Y, w):\n",
                "    '''\n",
                "    @params\n",
                "        X : 2D tensor of size(n,d)\n",
                "        Y : 1D tensor of size(n,1)\n",
                "        w : 1D tensor of size(d,1)\n",
                "    return derivative : 1D tensor of size(d,1)\n",
                "    '''\n",
                "    w = w.double()\n",
                "    #----------------TODO-----------------\n",
                "    derivative = None \n",
                "    return (derivative)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Task 4: Gradient Descent \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_model(X_train, Y_train, X_test, Y_test, w, eta):\n",
                "    '''\n",
                "    @params\n",
                "        X_train : 2D tensor over which model is trained\n",
                "        Y_train : 1D tensor over which model is trained\n",
                "        w : initial weights vector\n",
                "        eta : learning rate\n",
                "    @returns\n",
                "        w : final optimised w\n",
                "        test_err : list containing the l2-loss at each iteration\n",
                "    '''\n",
                "\n",
                "    epsilon = 1e-15  # Stopping precision\n",
                "    old_loss = 0\n",
                "    test_err = []  \n",
                "    \n",
                "   # -------------TODO---------------- \n",
                "   \n",
                "    return w, test_err\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Execution and Plotting\n",
                "Run the training model and visualize the results.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "eta = 1e-3\n",
                "w_init = torch.randn(d, 1)\n",
                "\n",
                "# Train\n",
                "w_trained, test_err = train_model(X_train, Y_train, X_test, Y_test, w_init, eta)\n",
                "\n",
                "# Plotting \n",
                "if len(test_err) > 0:\n",
                "    plt.plot(test_err)\n",
                "    plt.xlabel(\"Iterations\")\n",
                "    plt.ylabel(\"Test Error\")\n",
                "    plt.title(\"Test Error vs Iterations\")\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"Test error list is empty. Complete Task 4 to see the plot.\")\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "pytorch_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
